# -*- coding: utf-8 -*-
"""COVID -19 Viz & Data Prep

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ktSFIP-FCBhcRVJD4yh_lewOewkI3JHL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import boxcox
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge, Lasso
from scipy.special import inv_boxcox
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")

## read the datset from your local drive
from google.colab import files
upload = files.upload()

## upload the dataset
import pandas as pd
adf = pd.read_csv('africa_2022.csv')
df = adf[{'total_cases','new_cases','total_deaths','new_deaths','total_vaccinations','people_vaccinated','people_fully_vaccinated','total_boosters',
         'new_vaccinations'}]

## functions that you can use to learn your dataset
df.head(5)
df.info()
df.describe()

numeric_df = df.select_dtypes(include=['int64', 'float64'])
categorical_df = df.select_dtypes(include=['object'])

from pandas.core.arrays import categorical

numeric_df.columns
categorical_df.columns

numeric_df.shape
categorical_df.shape

from numpy.ma.core import correlate
######## In linear regression ######################################
## 6 points
### for continuous variables, what should I check first?

## A: For continuous variables you should check first if it is a instant variable or a ration variable

### Last time we mentioned that two situations will lead to fitting
### problems for LR, what are they?
### 1. overfitting
### 2. underfitting
### How to check 1 ?  What visualization technique can you use?
## Use the visualiation technique to create this graph

### Answer my previous question and write down the code below
numeric_df.corr()
plt.figure(figsize=(25,16))

## fill out the misssing code here
correlation = numeric_df.corr()
sns.heatmap(correlation, annot = True)
plt.show()



###################################################################
##### Univariate Data Analysis ####################################
fig=plt.subplots(figsize=(12, 21))
i=0
for feature in numeric_df.columns:
    if feature:
        i+=1
        plt.subplot(13, 3, i)
        sns.distplot(df[feature])
        plt.tight_layout()

## ToDO #############################################################
## Answer the below questions and fill up the code
## 6 points
######### How to find if each univariate variable has strong correlation with
######### the target variable?
##A: We can find it by seeing how big the area under the plot is which can tell us the bigger probability
## that the univariate has a strong correlation with the target variable
### What visualization technique should you use to explore this?
### create the plot here

fig=plt.subplots(figsize=(12, 21))
i=0
for feature in numeric_df.columns:
    if feature:
        i+=1
        plt.subplot(13, 3, i)
        ## fill out the misssing code here
        sns.scatterplot(numeric_df[feature], numeric_df['new_deaths'])


        plt.tight_layout()

## Check percentage of missing values
# Checking % of null values
for feat in df.columns:
    if df[feat].isnull().any():
        print(feat, ' : ',  round(df[feat].isnull().sum()/df.shape[0], 2)*100)

X = df.drop('new_deaths', axis = 1)

X.head()

y = df['new_deaths']
y.head()

#Check if y is a skewed distribution
sns.distplot(y)
print(y.skew())

### log transformation to remove skewness
## fill out the code here
y_bc = np.log(y)

from sklearn.model_selection import train_test_split
### ToDO################################################################################
## 2 points
## split X, y_bc into training and testing sets with training proportion as 0.8
## set random_state=100
## fill out the code here
X_train, X_test, y_train, y_test = train_test_split(X, y_bc, test_size=0.2, shuffle=True, random_state=(100))

X_test.shape

y_test.shape

X_train.shape

X_train.head()

def null_values(dataf):
    for feat in df.columns:
        if dataf[feat].isnull().any():
            print(feat+" : "+str(dataf[feat].isnull().sum()))
### ToDO #########################################################

X_test.head()

X_train['new_vaccinations'].dtype

#Missing value imputation
## Imputing missing values with mean of train data since outliers are already removed.
TVimputed_value = np.mean(X_train['total_vaccinations'])
NVimputed_value = np.mean(X_train['new_vaccinations'])
PVimputed_value = np.mean(X_train['people_vaccinated'])
TBimputed_value = np.mean(X_train['total_boosters'])
PVCimputed_value = np.mean(X_train['people_fully_vaccinated'])



print("Mean : ",TVimputed_value)
print("Mean : ",NVimputed_value)
print("Mean : ",PVimputed_value)
print("Mean : ",TBimputed_value)
print("Mean : ",PVCimputed_value)




### Replace the missing values with the mean of the training data in both
##################################################################
### training set and testing set
### use fillna() function and what value should you assign to 'inplace' argument?
X_train['total_vaccinations'].fillna(TVimputed_value, inplace = True)
X_test['total_vaccinations'].fillna(TVimputed_value, inplace = True)
X_train['new_vaccinations'].fillna(NVimputed_value, inplace = True)
X_test['new_vaccinations'].fillna(NVimputed_value, inplace = True)
X_train['people_vaccinated'].fillna(PVimputed_value, inplace = True)
X_test['people_vaccinated'].fillna(PVimputed_value, inplace = True)
X_train['total_boosters'].fillna(TBimputed_value, inplace = True)
X_test['total_boosters'].fillna(TBimputed_value, inplace = True)
X_train['people_fully_vaccinated'].fillna(PVCimputed_value, inplace = True)
X_test['people_fully_vaccinated'].fillna(PVCimputed_value, inplace = True)

#a = X_train.columns
X_train.fillna( 0 ,inplace=True)
X_test.fillna( 0 ,inplace=True)
#y_train = y_train.replace(np.nan, 0)

X_test.shape

y_test.shape

X_test.head()

#y_train.to_csv("y_train.csv")
#files.download("y_train.csv")

X_train['total_deaths'] = pd.to_numeric(X_train['total_deaths'],errors = 'coerce')

#X_Train['total_deaths'] = X_train['total_deaths'].astype(float)
X_train['new_cases'] = X_train['new_cases'].astype(float)
X_train['total_cases'] = X_train['total_cases'].astype(float)

X_train.head()

y_train = y_train.replace([-np.Inf, np.nan], 0)
y_test = y_test.replace([-np.Inf, np.nan], 0)
X_train = X_train.replace([-np.Inf, np.nan], 0)
X_test = X_test.replace([-np.Inf, np.nan], 0 )



"""y_test is prohibiintg me from getting the rmse value as it has inconsitant numbers compared to x_test. i commented that line out as it chnages from having -inf value erroe or its inconsistent numbers and gives that error"""

y_test.shape

X_test.shape

display(y_test)

a =X_train.dtypes
print(a)

X_train.shape

X_train.head()

y_test.shape

### scaling the features
sscaler = StandardScaler()

###############################################################
#### ToDO #####################################################
## check the documentation of this StandardScaler() class
## https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html

### 1 point
## What does fit() function do?
# the fit() function fits the X_train data to the model.



sscaler.fit(X_train.values.reshape(-1,1))

## 1 point
## what does transform() function do?
# the transform() function returns a dataframe with transformed values that reflect parameters.
X_train_sc = sscaler.fit_transform(X_train)
X_test_sc = sscaler.fit_transform(X_test)
#y_train_sc = sscaler.fit_transform(y_train.values.reshape(-1,1),)
#y_test_sc = sscaler.fit_transform(y_test.values.reshape(-1,1),)



#X_train_sc = pd.DataFrame(data=sscaler.transform(X_train.values.reshape(-1,1)), columns=X_train.columns)
#X_test_sc = pd.DataFrame(data=sscaler.transform(X_test), columns=X_test.columns)

check_for_nan = X_test.isnull().values.any()
print (check_for_nan)

X_test_sc.shape

## Fit a Lasso Regression
#from sklearn.linear_model import Ridge, Lasso



lasso = Lasso(alpha=0.1)
lasso.fit(X_train,y_train)


y_train_pred = lasso.predict(X_train_sc)
y_test_pred =  lasso.predict(X_test_sc)

### Model evaluation
print('R2 score (train) : ',round(r2_score(y_train,y_train_pred),4))
print('R2 score (test) : ',round(r2_score(y_test,y_test_pred),4))
print('RMSE (train) : ', round(np.sqrt(mean_squared_error(y_train, y_train_pred)),4))
print('RMSE (test) : ', round(np.sqrt(mean_squared_error(y_test, y_test_pred)),4))

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split


reg = RandomForestRegressor(criterion='mse')
reg.fit(X_train, y_train)

modelPred = reg.predict(X_test)

print(modelPred)
print("Number of predictions:",len(modelPred))

meanSquaredError = mean_squared_error(y_test, modelPred)

print("MSE:", meanSquaredError)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)
print("Test set prediction:\n:", y_pred)



######################### ToDO #################################################
# evaluate the model in terms of accuracy
# 2 points
y_test=y_pred
# obtain the accuracy of the fitted model for the test datast
accuracy = np.mean(y_test==y_pred)
print("Test set score:", accuracy)

print("Test set score: {:.2f}".format(accuracy))

score = knn.score(X_test, y_test)
print("Test set score : {:.2f}".format(score))