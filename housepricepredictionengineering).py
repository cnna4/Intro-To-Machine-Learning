# -*- coding: utf-8 -*-
"""HousePricePredictionEngineering).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13XBONnO3-oIZs6rbL9swaY9hZ03fbsP4

# Assignment 3

This assignment mainly enhances your **Feature Engineering** Skills. The total mark for this assignment is 20. Answer the questions in the code chunk and
fill up the missing part of the code. The tentative due date for this assignment
is 11:59 PM, Wednesday, March 2nd. If I fall behind my schedule, extensions will be given.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import boxcox
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge, Lasso
from scipy.special import inv_boxcox
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")

## read the datset from your local drive
from google.colab import files
upload = files.upload()

## upload the dataset
import pandas as pd
df = pd.read_csv('train.csv')
df.shape

"""# Feature Engineering"""

###### ToDO ##########################################################
### 1 point
# Removing one of the high correlated from pairs checked using heatmap
removeFeats = ['GarageCars', 'GarageYrBlt', 'GrLivArea', 'TotalBsmtSF', 'BedroomAbvGr']
## use .drop() function of pandas dataframe,
## I will need to remove columns, what value should I give for axis?
## do not forget about setting inplace=True
## Fill out the code here

# From EDA, remove numerical features having 1 unique value as occuring more than 99%
toDrop= ['BsmtFinSF2', 'LowQualFinSF', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch','ScreenPorch','PoolArea','MiscVal']
numeric_df = df.select_dtypes(include=['int64', 'float64'])
categorical_df = df.select_dtypes(include=['object'])
numeric_df['BsmtFinSF2'].value_counts(normalize=True, sort=True, ascending=False)[0]
## ToDO###############################################################
## 2 points
######################################################################
for c in toDrop:
    ## fill out the code here
    if float() >= 0.99:
    ## fill out the code here

print(df.columns.shape)

## Check percentage of missing values
# Checking % of null values
for feat in df.columns:
    if df[feat].isnull().any():
        print(feat, ' : ',  round(df[feat].isnull().sum()/df.shape[0], 2)*100)

# Since MasVnrArea has only 1% data missing, dropping rows with NULL values in MasVnrArea
## ToDO #################################
### 1 points
## fill out the code here


# Dropping Id column as it does not contribute towards predicting SalePrice
## Once this line has been run, you need to comment out this line of code
df.drop(['Id'], axis=1, inplace=True)

df.shape

print(df['Electrical'].isnull().sum())

## ToDO###########################################################################
## 1 point
# dropping rows with null values in 'Electrical', for very low missing value count
## using dropna function
## fill out the code here

# dropping 'PoolQC' for very high percentage of missing value and
## highly imbalance data (if missing value is imputed)
df.drop(['PoolQC'], axis=1, inplace=True)

"""# Should I remove the MiscFeatures and Alley features
Information:

For 'Alley', Nan means 'No access to alley'

For 'BsmtQual', 'BsmtCond', BsmtExposure, BsmtFinType1, BsmtFinType2 Nan means 'No basement'

For GarageType, GarageFinish, GarageQual, GarageCond Nan means 'No garage'

For 'FireplaceQu' and 'Fence' Nan means 'No Fire place' and 'No fence' respectively

MiscFeature - Nan means no additional features mentioned.

All these features can be imputed by making them into one category in place of missing data.
"""

# Some categorical features have NAN values which denotes to a particular class.

impute_cat_features = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \
                       'BsmtFinType1', 'BsmtFinType2', 'GarageType', 'GarageFinish',\
                       'GarageQual', 'GarageCond', 'FireplaceQu' ,'Fence' ,'MiscFeature']
print(df[impute_cat_features].isnull().sum())

## ToDO ###############################################################
## 1 point, fill out the NA values as string 'NA_'+ feature names to make it a category
for feat in impute_cat_features:
    ### fill out the code here

### Encoding ordinal categorical features
df['ExterQual'] = df['ExterQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})
df['ExterCond'] = df['ExterCond'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})
df['BsmtQual'] = df['BsmtQual'].map({'NA_BsmtQual':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})
df['BsmtCond'] = df['BsmtCond'].map({'NA_BsmtCond':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})
df['BsmtExposure'] = df['BsmtExposure'].map({'NA_BsmtExposure':0,'No':1,'Mn':2,'Av':3,'Gd':4})
df['BsmtFinType1'] = df['BsmtFinType1'].map({'NA_BsmtFinType1':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})
df['BsmtFinType2'] = df['BsmtFinType2'].map({'NA_BsmtFinType2':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})
df['HeatingQC'] = df['HeatingQC'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})
df['KitchenQual'] = df['KitchenQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})
df['GarageFinish'] = df['GarageFinish'].map({'NA_GarageFinish':0,'Unf':1,'RFn':2,'Fin':3})
df['GarageQual'] = df['GarageQual'].map({'NA_GarageQual':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})
df['GarageCond'] = df['GarageCond'].map({'NA_GarageCond':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})
df['FireplaceQu'] = df['FireplaceQu'].map({'NA_FireplaceQu':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})

df.head()

df.shape

### One hot encoding on nominal features
unordered_features = ['MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1' , 'Condition2',
                      'BldgType', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating',
                      'Electrical', 'GarageType','PavedDrive', 'Fence', 'MiscFeature', 'SaleType','SaleCondition','LotShape',
                      'Utilities', 'LandSlope', 'HouseStyle', 'CentralAir', 'Functional']

##################################################################################
#### ToDO ########################################################################
## use pd.get_dummies to apply one hot encoding on nomial features and set
## drop_first to True, name the new dataframe as dummy_df
## 1 point
### fill out the code here

dummy_df.shape

# Dropping those classes which are present less than/equal to 1% of the observations.
dummy_cols_drop = []
for feat in dummy_df.columns:
    if dummy_df[feat].value_counts()[0]/dummy_df.shape[0] >= 0.98:
        dummy_cols_drop.append(feat)
    elif dummy_df[feat].value_counts()[1]/dummy_df.shape[1] >= 0.98:
        dummy_cols_drop.append(feat)

print(dummy_cols_drop)
print(len(dummy_cols_drop))

## drop these classes
dummy_df.drop(columns = dummy_cols_drop, axis = 1, inplace = True)
dummy_df.shape

###### To do #########################################
### 1 point
# Appending the dummy variables to the original dataframe
## fill out the code here



# Dropping the redundant columns
df = df.drop(unordered_features,axis=1)

## Obtain the feature matrix
X = df.drop('SalePrice', axis = 1)

X.head()

y = df['SalePrice']
y.head()

## Check if y is a skewed distribution
sns.distplot(y)
print(y.skew())

### ToDO ###############################################
## 1 point
### log transformation to remove skewness
## fill out the code here

### draw the histogram of y after the log transformation
sns.distplot(y_bc)

from sklearn.model_selection import train_test_split
### ToDO################################################################################
## 2 points
## split X, y_bc into training and testing sets with training proportion as 0.8
## set random_state=100
## fill out the code here

X_train.shape

X_train.head()

#### Outlier treatment
#### Checking percentage of outliers in each continuous numeric feature
outliers_percentage={}
numeric_df = X_train.select_dtypes(include=['int64', 'float64'])
cols = ['SalePrice','YearBuilt','OverallCond', 'OverallQual', 'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',
           'FullBath', 'MoSold', 'Yrsold', 'BsmtQual', 'ExterCond','BsmtExposure', 'BsmtFinType2','BsmtCond',
            'TotRmsAbvGrd','GarageCond','GarageQual', 'KitchenAbvGr', 'LowQualFinSF','Fireplaces']
for feature in numeric_df.columns:
    if feature not in cols:
        IQR=numeric_df[feature].quantile(.75)-numeric_df[feature].quantile(.25)
        outliers_count=numeric_df[(numeric_df[feature]>(numeric_df[feature].quantile(.75)+1.5*IQR)) | (numeric_df[feature]<(numeric_df[feature].quantile(.25)-1.5*IQR))].shape[0]
        outliers_percentage[feature]=round(outliers_count/numeric_df.shape[0]*100,2)

outlier_df=pd.DataFrame({'Features':list(outliers_percentage.keys()),'Percentage':list(outliers_percentage.values())})
outlier_df.sort_values(by="Percentage", ascending=False)

################################################################################
#### ToDO ######################################################################
### A total of  5 points
# Outlier Treatment
for feature, percent in outliers_percentage.items():
    if percent > 0:
        ### find the IQR of each feature (1 point)


        ### find the maximum upper fence point of a box plot, which is
        ### the 75% quantile + 1.5*IQR (1 point)


        ### find the minimum upper fence point of a box plot, which is
        ### the 75% quantile - 1.5*IQR (1 point)


        ## replace all the observations of feature that is larger than
        ## max_value with max_value in both training set and testing set (1 point)


        ## replace all the observations of feature that is larger than
        ## min_value with min_value in both training set and testing set (1 point)


        print(feature, IQR, min_value, max_value)

"""# Remark
Many features have outliers

Dropping all the outliers will cause loss of information.

Hence reassigning fixed minimum and maximum values to those rows where feature value is outside the range of [25th percentile - 1.5 IQR, 75th percentile + 1.5 IQR]
IQR or Inter Quartile Range = Difference between 75th percentile and 25th percentile values of a feature.

Target column 'SalePrice' is excluded in this since this is our target variable(Important point)!

Some other features are also excluded since those are ordered categorical type which are labelled encoded to numeric form.
"""

#  Checking Null values
## python function check NULL values for each column of a data frame
def null_values(dataf):
    for feat in dataf.columns:
        if dataf[feat].isnull().any():
            print(feat+" : "+str(dataf[feat].isnull().sum()))
### ToDO ######################################################################
### 1 point, asnwer this question
## What is the meaning of this line str(dataf[feat].isnull().sum()) (1 point)

##### ToDO ######################################
### 1 point
### Apply null_values function to X_train and X_test
null_values(X_train)
print("-----------")
null_values(X_test)

X_train['LotFrontage'].dtype

# Missing value imputation
## Imputing missing values with mean of train data since outliers are already removed.
imputed_value = np.mean(X_train['LotFrontage'])
print("Mean : ",imputed_value)
### Replace the missing values with the mean of the training data in both
##################################################################
### training set and testing set
### use fillna() function and what value should you assign to 'inplace' argument?
X_train['LotFrontage'].fillna(imputed_value, inplace = True)
X_test['LotFrontage'].fillna(imputed_value, inplace = True)

X_train.head()

X_test.head()

#### Last step ##########################################
### scaling the features
sscaler = StandardScaler()

###############################################################
#### ToDO #####################################################
## check the documentation of this StandardScaler() class
## https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
### 1 point
## What does fit() function do?
sscaler.fit(X_train)
## 1 point
## what does transform() function do?
X_train_sc = pd.DataFrame(data=sscaler.transform(X_train), columns=X_train.columns)
X_test_sc = pd.DataFrame(data=sscaler.transform(X_test), columns=X_test.columns)

X_train_sc.head()

X_test_sc.head()

### saved the cleaned datast X_train_sc, X_test_sc, y_train and y_test to
### your local folder
from google.colab import files
X_train_sc.to_csv("X_train_sc.csv")
X_test_sc.to_csv("X_test_sc.csv")
y_train.to_csv("y_train.csv")
y_test.to_csv("y_test.csv")
files.download("X_train_sc.csv")
files.download("X_test_sc.csv")
files.download("y_train.csv")
files.download("y_test.csv")